{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Create main.py"
      ],
      "metadata": {
        "id": "Rbn48ZKDLk-6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "from PyPDF2 import PdfReader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from groq import Groq\n",
        "import os\n",
        "import pickle\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables from .env file\n",
        "load_dotenv()\n",
        "\n",
        "@st.cache_resource\n",
        "def initialize_groq_llm():\n",
        "    # Initialize the Groq language model with API key from environment\n",
        "    api_key = os.getenv(\"GROQ_API_KEY\")\n",
        "    if not api_key:\n",
        "        st.error(\"API key is missing. Please check your .env file.\")\n",
        "        return None\n",
        "    return Groq(api_key=api_key)\n",
        "\n",
        "def load_vector_store(pdf_path, embeddings, store_name):\n",
        "    # Load existing vector store or create a new one from the PDF\n",
        "    if os.path.exists(f\"{store_name}.pkl\"):\n",
        "        with open(f\"{store_name}.pkl\", \"rb\") as f:\n",
        "            return pickle.load(f)\n",
        "    else:\n",
        "        # Extract text from PDF and create vector store\n",
        "        pdf_reader = PdfReader(pdf_path)\n",
        "        text = \"\".join([page.extract_text() for page in pdf_reader.pages])\n",
        "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200, length_function=len)\n",
        "        chunks = text_splitter.split_text(text=text)\n",
        "        vector_store = FAISS.from_texts(chunks, embedding=embeddings)\n",
        "        with open(f\"{store_name}.pkl\", \"wb\") as f:\n",
        "            pickle.dump(vector_store, f)\n",
        "        return vector_store\n",
        "\n",
        "def main():\n",
        "    # Main function to run the Streamlit app\n",
        "    st.title(\"Simple RAG Application\")\n",
        "\n",
        "    llm = initialize_groq_llm()\n",
        "    if llm is None:\n",
        "        return  # Exit if the API key is not available\n",
        "\n",
        "    pdf_path = \"document.pdf\"\n",
        "\n",
        "    # Define embedding models\n",
        "    embeddings_models = {\n",
        "        '300-dim': HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\"),\n",
        "        '700-dim': HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-MiniLM-L6-v2\"),\n",
        "        '1536-dim': HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
        "    }\n",
        "\n",
        "    # Load vector stores for each embedding model\n",
        "    vector_stores = {}\n",
        "    for name, embeddings in embeddings_models.items():\n",
        "        vector_stores[name] = load_vector_store(pdf_path, embeddings, f\"vector_store_{name}\")\n",
        "\n",
        "    query = st.text_input(\"**Ask your question:**\")\n",
        "\n",
        "    if query:\n",
        "        responses = {}\n",
        "        # Search and get responses from each vector store\n",
        "        for name, vector_store in vector_stores.items():\n",
        "            st.write(f\"Searching in vector store: {name}\")\n",
        "            docs = vector_store.similarity_search(query=query, k=3)\n",
        "\n",
        "            if not docs:\n",
        "                st.write(f\"No documents found in {name} vector store.\")\n",
        "                responses[name] = \"No documents found.\"\n",
        "                continue\n",
        "\n",
        "            st.write(f\"Found {len(docs)} documents in {name} vector store.\")\n",
        "            snippets = \" \".join([doc.page_content for doc in docs])\n",
        "            st.write(f\"Document snippets: {snippets}\")\n",
        "\n",
        "            # Generate a response from the Groq language model\n",
        "            prompt = f\"Given the following document snippets, provide a detailed and relevant response to the query: '{query}'.\\n\\nDocument Snippets:\\n{snippets}\"\n",
        "            try:\n",
        "                result = llm.chat.completions.create(\n",
        "                    model=\"llama3-70b-8192\",\n",
        "                    messages=[\n",
        "                        {\"role\": \"system\", \"content\": \"Provide detailed and clear responses based on the provided document snippets.\"},\n",
        "                        {\"role\": \"user\", \"content\": prompt}\n",
        "                    ]\n",
        "                )\n",
        "                response_content = result.choices[0].message.content\n",
        "            except Exception as e:\n",
        "                st.write(f\"Error occurred: {e}\")\n",
        "                response_content = \"An error occurred while generating the response.\"\n",
        "\n",
        "            responses[name] = response_content\n",
        "\n",
        "        # Display responses from different embedding models\n",
        "        st.subheader(\"Responses from Different Embeddings:\")\n",
        "        for name, response in responses.items():\n",
        "            st.write(f\"**{name}**:\")\n",
        "            st.write(response)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "asdssgJ-ImiF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "create a requirements.txt"
      ],
      "metadata": {
        "id": "XxERY6AFLjkH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "streamlit\n",
        "PyPDF2\n",
        "langchain\n",
        "langchain-huggingface\n",
        "langchain-community\n",
        "groq\n",
        "python-dotenv\n",
        "faiss-gpu"
      ],
      "metadata": {
        "id": "A7CyJmW2I5Sq",
        "outputId": "138e7185-a964-4b2e-ea7d-96b69487b3c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'streamlit' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-be9776746df9>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstreamlit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mPyPDF2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mhuggingface\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mcommunity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'streamlit' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "create a .env file"
      ],
      "metadata": {
        "id": "sBsCO-isMWHF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GROQ_API_KEY=\"key\"\n"
      ],
      "metadata": {
        "id": "NOLsVhBWJCFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "create a .gitignore file"
      ],
      "metadata": {
        "id": "DRxq0AsWMZWd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ".env"
      ],
      "metadata": {
        "id": "BVbZajUwJFtW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "store the document that you want to use in application and rename to document.pdf"
      ],
      "metadata": {
        "id": "QDidG9g6JJLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "pip install -r requirments.txt\n"
      ],
      "metadata": {
        "id": "JTtNw6pwPuK2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "streamlit run main.py\n"
      ],
      "metadata": {
        "id": "_PHOvMlep1nf"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}